{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from typing import Iterator\n",
    "from langchain_core.documents import Document\n",
    "from paddleocr import PaddleOCR\n",
    "from pprint import pprint\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder_if_not_exists(folder_path:str):  # 이미지 저장 폴더 생성\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"폴더가 생성되었습니다: {folder_path}\")\n",
    "    else:\n",
    "        print(f\"폴더가 이미 존재합니다: {folder_path}\")\n",
    "\n",
    "def save_pdf_to_img(path:str, file_name:str, page_num:int):  # pdf를 png 이미지 파일로 저장\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        page = pdf.pages[page_num]\n",
    "        im = page.to_image(resolution=150)\n",
    "        # im.draw_rects(first_page.extract_words())  # 글자에 Red Box 그리기\n",
    "        save_path = f\"{os.getcwd()}/images/{file_name}/{file_name}_{page_num}.png\"\n",
    "        im.save(save_path, format=\"PNG\", )\n",
    "    return save_path\n",
    "\n",
    "table_settings={      # extract_tables method variable (깃헙 디폴트 세팅 참조)\n",
    "    \"vertical_strategy\": \"lines\",\n",
    "    \"horizontal_strategy\": \"lines\",\n",
    "    \"explicit_vertical_lines\": [],\n",
    "    \"explicit_horizontal_lines\": [],\n",
    "    \"snap_tolerance\": 3,\n",
    "    \"snap_x_tolerance\": 3,\n",
    "    \"snap_y_tolerance\": 3,\n",
    "    \"join_tolerance\": 3,\n",
    "    \"join_x_tolerance\": 3,\n",
    "    \"join_y_tolerance\": 3,\n",
    "    \"edge_min_length\": 3,\n",
    "    \"min_words_vertical\": 3,\n",
    "    \"min_words_horizontal\": 1,\n",
    "    \"intersection_tolerance\": 3,\n",
    "    \"intersection_x_tolerance\": 3,\n",
    "    \"intersection_y_tolerance\": 3,\n",
    "    \"text_tolerance\": 3,\n",
    "    \"text_x_tolerance\": 3,\n",
    "    \"text_y_tolerance\": 3,\n",
    "    # \"text_*\": …,\n",
    "    }\n",
    "\n",
    "def convert_header_to_separator(header: str) -> str:   # 테이블 첫줄 파싱후, 두번째 줄에 Header Line 추가 함수(마크다운 형식을 위한)\n",
    "    # Use a regex to replace each header content with the appropriate number of hyphens\n",
    "    separator = re.sub(r'[^|]+', lambda m: '-' * max(1, len(m.group(0))), header) # max 부분 관련 구분자는 최소 1개는 들어가야 마크다운 적용\n",
    "    separator = separator.replace(\"||\", \"|-|\", 1)  # 수평구분자가 최소 한개는 있어야 마크다운 적용(그냥 비어있으면 안됨)\n",
    "    return separator\n",
    "\n",
    "def table_parser(pdf_path:str, page_num:int, crop:bool=False) -> list:   # 테이블 파싱(마크다운 형식), A4상단 표준 크롭핑 적용 선택 가능(디폴트 false)\n",
    "    full_table = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # Find the examined page\n",
    "        table_page = pdf.pages[page_num]\n",
    "        if crop:\n",
    "            bounding_box = (3, 70, 590, 770)   #default : (0, 0, 595, 841)\n",
    "            table_page = table_page.crop(bounding_box, relative=False, strict=True)\n",
    "        else: pass\n",
    "        tables = table_page.extract_tables(table_settings = table_settings)\n",
    "        # if tables:\n",
    "        for table in tables:\n",
    "            table_string = ''\n",
    "            # Iterate through each row of the table\n",
    "            for row_num in range(len(table)):\n",
    "                row = table[row_num]\n",
    "                # Remove the line breaker from the wrapped texts\n",
    "                cleaned_row = [item.replace('\\n', ' ') if item is not None and '\\n' in item else 'None' if item is None else item for item in row]\n",
    "                # Convert the table into a string\n",
    "                table_string+=('|'+'|'.join(cleaned_row)+'|'+'\\n')\n",
    "                if row_num ==0:  # 첫줄 작업이면, Header Line 추가\n",
    "                    header_line = convert_header_to_separator(table_string[:-1])\n",
    "                    table_string+= header_line+'\\n'\n",
    "            # Removing the last line break\n",
    "            table_string = table_string[:-1]\n",
    "            full_table.append(table_string)\n",
    "        return full_table\n",
    "\n",
    "def extract_level_name(path:str) -> list:  # 폴더 구조(lv1, lv2, lv3를 metadata로 추출하는 함수)\n",
    "    temp = path.split(\"/\")  # path 예시 : ['.\\\\2024\\\\Manual\\\\Guidance for Autonomous Ships_2023.pdf','.\\\\2024\\\\POS\\\\FWG.pdf']\n",
    "    lv1 = temp[1]\n",
    "    if temp[2]:\n",
    "        if temp[2] != temp[-1]:\n",
    "            lv2 = temp[2]\n",
    "            lv3 = temp[-1].replace(\".pdf\", \"\")\n",
    "        else:\n",
    "            lv2 = None\n",
    "            lv3 = temp[-1].replace(\".pdf\", \"\")\n",
    "    result = [lv1, lv2, lv3]\n",
    "    return result\n",
    "\n",
    "total_results =[]\n",
    "def main_filepath_extractor(path:str) -> list:   # 폴더 트리를 리커시브하게 읽어서 전체 PDF 파일의 full 경로를 리스트에 수집\n",
    "    global total_results\n",
    "    all_items = os.listdir(path)\n",
    "    files = [f for f in all_items if os.path.isfile(os.path.join(path, f))]\n",
    "    results = [os.path.join(path, file) for file in files]\n",
    "    results = [result.replace(\"\\\\\", \"/\") for result in results]\n",
    "    total_results.extend(results)\n",
    "    dirs = [f for f in all_items if os.path.isdir(os.path.join(path, f))]\n",
    "    if dirs:\n",
    "        dirs = [path+\"/\" + lv2_dir for lv2_dir in dirs]\n",
    "        for dir in dirs:\n",
    "            main_filepath_extractor(dir)\n",
    "    return total_results\n",
    "\n",
    "def main_parser(path:str, crop:bool=False, lang:str=\"en\") -> Iterator[Document]:  # 메인 Parsing 함수, text-extraction은 pypdf2 적용\n",
    "    '''\n",
    "    - pdfplumber: table, image 추출\n",
    "    - pypdf2: text 추출\n",
    "    - paddleocr : 이미지 pdf 줄파싱\n",
    "    - lang 후보: ['ch', 'en', 'korean', 'japan', 'chinese_cht', 'ta', 'te', 'ka', 'latin', 'arabic', 'cyrillic', 'devanagari']\n",
    "    '''\n",
    "    full_result = []\n",
    "    file_name = path.split(\"/\")[-1].split(\".\")[0].strip()\n",
    "    img_save_folder = os.path.join(os.getcwd(), f\"images/{file_name}\")  # images 폴더 생성후 그 안에 file_name폴더 생성\n",
    "    create_folder_if_not_exists(img_save_folder)  # 이미지 저장할 폴더 생성\n",
    "    ocr = PaddleOCR(use_angle_cls=True, lang=lang)\n",
    "\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        page_number = 0  # for metadata\n",
    "        for _ in tqdm(pdf.pages):\n",
    "            level_names = extract_level_name(path)  # for metadata\n",
    "            img_path = save_pdf_to_img(path, file_name, page_number) # for saving pdf page as png img file\n",
    "            reader = PdfReader(path)\n",
    "            page = reader.pages[page_number]\n",
    "            text_result = page.extract_text().replace(\"\\n\", \" \").replace(\"- \", \"\").replace(\"  \", \" \")\n",
    "\n",
    "            if len(text_result) == 0:  # 텍스트 추출 결과가 없으면, OCR 실시\n",
    "                print(\"이미지 OCR\")\n",
    "                ocr_result = ocr.ocr(img_path)\n",
    "                for idx in range(len(ocr_result)):\n",
    "                    res = ocr_result[idx]\n",
    "                    temp_result = []\n",
    "                    try:\n",
    "                        for line in res:\n",
    "                            temp_result.append(line[1][0])\n",
    "                    except: temp_result.append(\"Error has been occured\")\n",
    "                text_result = \" \".join(temp_result)\n",
    "\n",
    "            table_result = table_parser(path, page_number, crop)  # for page_content\n",
    "\n",
    "            if table_result:\n",
    "                total_page_result = \"\"\n",
    "                for table in table_result:\n",
    "                    total_page_result = text_result + \"\\n\\n\" + table   # table_result가 있으면, text_result 끝에 엔터후 이어붙이기\n",
    "                    result = Document(\n",
    "                        page_content=total_page_result,\n",
    "                        metadata={\"Page\": page_number, \"First Division\":level_names[0], \"Second Division\": level_names[1], \"File Name\": level_names[2], \"File Path\": path},\n",
    "                        )\n",
    "            else:\n",
    "                result = Document(\n",
    "                    page_content = text_result,\n",
    "                    metadata={\"Page\": page_number, \"First Division\":level_names[0], \"Second Division\": level_names[1], \"File Name\": level_names[2], \"File Path\": path},\n",
    "                    )\n",
    "            full_result.append(result)\n",
    "            page_number += 1\n",
    "        parsed_document = full_result\n",
    "    return parsed_document   # langchain Document type\n",
    "### [End] Main Fucntions with pdfminer.six ###########################################################################################\n",
    "\n",
    "def add_firstline_in_splitted_text(origin_splitted_text:str):\n",
    "    lv1 = origin_splitted_text.metadata[\"First Division\"]\n",
    "    lv2 = origin_splitted_text.metadata[\"Second Division\"]\n",
    "    title = origin_splitted_text.metadata[\"File Name\"]\n",
    "    origin_page_content = origin_splitted_text.page_content\n",
    "    first_sentence = f\"This page explains {title}, that belongs to catogories of {lv1} and {lv2}.\"\n",
    "    new_page_content = f'''{first_sentence}/n{origin_page_content}'''\n",
    "    origin_matadata = origin_splitted_text.metadata\n",
    "    return Document(page_content=new_page_content, metadata=origin_matadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "def read_vectordb_as_df(db_path:str):\n",
    "    result = []\n",
    "    client = chromadb.PersistentClient(path=db_path)\n",
    "    for collection in client.list_collections():\n",
    "        data = collection.get(include=['embeddings','documents', 'metadatas'])\n",
    "        result.append(data)\n",
    "        df = pd.DataFrame({\"ids\":data[\"ids\"],\n",
    "                           \"metadatas\":data[\"metadatas\"],\n",
    "                           \"documents\":data[\"documents\"]})\n",
    "        df[\"first_div\"] = df[\"metadatas\"].apply(lambda x: x[\"First Division\"])\n",
    "        df[\"second_div\"] = df[\"metadatas\"].apply(lambda x: x[\"Second Division\"])\n",
    "        df[\"filename\"] = df[\"metadatas\"].apply(lambda x: x[\"File Name\"])\n",
    "        df = df[[\"ids\", \"first_div\", \"second_div\",\"filename\",\"documents\", \"metadatas\"]]\n",
    "    return df\n",
    "\n",
    "\n",
    "def delete_document(filename:str, db_path:str):\n",
    "  vector_store = Chroma(collection_name=\"collection_01\", persist_directory=db_path, embedding_function=OllamaEmbeddings(model=\"bge-m3:latest\"))\n",
    "  del_ids = vector_store.get(where={'File Name':filename})[\"ids\"]\n",
    "  vector_store.delete(del_ids)\n",
    "  print(\"Document is deleted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lv1_dir = \"./Rules\"     # 최상단 엄마 폴더\n",
    "db_path = \"./db/chroma_db_03\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./Rules/DNV/DNV Rules for Classification of Ships _2016_39_Fishing vessels.pdf', './Rules/DNV/DNV Rules for Classification of Ships _2016_40_Offshore Service Vessels, Tugs and Special Ships.pdf', './Rules/DNV/DNV Rules for Classification of Ships _2016_41_Slop reception and processing facilities.pdf', './Rules/DNV/DNV Rules for Classification of Ships _2016_42_Ships for Carriage of Refrigerated Cargoes.pdf', './Rules/DNV/DNV Rules for Classification of Ships _2016_43_Carriage of Dangerous Goods.pdf']\n",
      "5\n",
      "['Rules', 'DNV', 'DNV Rules for Classification of Ships _2016_43_Carriage of Dangerous Goods']\n"
     ]
    }
   ],
   "source": [
    "total_paths = []\n",
    "total_paths = main_filepath_extractor(path=lv1_dir)  # 모든 PDF의 Full Path를 리스트에 담기\n",
    "total_paths = list(set(total_paths))\n",
    "total_paths.sort()\n",
    "print(total_paths)\n",
    "print(len(total_paths))\n",
    "res = extract_level_name(path=total_paths[-1])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 중복 파일 체크\n",
      "2\n",
      "DNV Rules for Classification of Ships _2016_39_Fishing vessels\n",
      "Already Parsed Document\n",
      ">>> 중복 파일 체크\n",
      "2\n",
      "DNV Rules for Classification of Ships _2016_40_Offshore Service Vessels, Tugs and Special Ships\n",
      "Already Parsed Document\n",
      ">>> 중복 파일 체크\n",
      "2\n",
      "DNV Rules for Classification of Ships _2016_41_Slop reception and processing facilities\n",
      "============= MAIN PARSER ============\n",
      "폴더가 생성되었습니다: d:\\AA_develop\\parsing\\images/DNV Rules for Classification of Ships _2016_41_Slop reception and processing facilities\n",
      "[2024/10/28 12:15:49] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='C:\\\\Users\\\\jongb/.paddleocr/whl\\\\det\\\\en\\\\en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='C:\\\\Users\\\\jongb/.paddleocr/whl\\\\rec\\\\en\\\\en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='d:\\\\AA_develop\\\\parsing\\\\myenv_310\\\\lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='C:\\\\Users\\\\jongb/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:02<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Text Splitter - 28============\n",
      "============= Embedding  ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "for path in tqdm(total_paths):\n",
    "  print(\">>> 중복 파일 체크\")\n",
    "  vector_store = Chroma(collection_name=\"collection_01\", persist_directory=db_path, embedding_function=OllamaEmbeddings(model=\"bge-m3:latest\"))\n",
    "  df = read_vectordb_as_df(db_path=db_path)\n",
    "  vectordb_filenames = df[\"filename\"].unique().tolist()\n",
    "  target_filename = extract_level_name(path=path)[-1]\n",
    "\n",
    "  print(len(vectordb_filenames))\n",
    "  print(target_filename)\n",
    "\n",
    "  if target_filename not in vectordb_filenames:\n",
    "\n",
    "    print(\"============= MAIN PARSER ============\")\n",
    "    parsed_text = main_parser(path=path, lang=\"en\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "    splitted_texts = text_splitter.split_documents(parsed_text)\n",
    "    new_splitted_texts = [add_firstline_in_splitted_text(text) for text in splitted_texts]\n",
    "    print(f\"============= Text Splitter - {len(new_splitted_texts)}============\")\n",
    "\n",
    "    vector_store = Chroma(collection_name=\"collection_01\",embedding_function=OllamaEmbeddings(model=\"bge-m3:latest\"), persist_directory=db_path)\n",
    "\n",
    "    print(\"============= Embedding  ============\")\n",
    "\n",
    "    # squares_generator = (i for i in new_splitted_texts)\n",
    "    # for div in tqdm(squares_generator):\n",
    "    #   vector_store.add_documents(documents=[div])\n",
    "\n",
    "    for div in tqdm(new_splitted_texts):\n",
    "      vector_store.add_documents(documents=[div])\n",
    "\n",
    "    print(f\">>> [End]{path}--------------------------------------------\")\n",
    "    print(\"\")\n",
    "\n",
    "  else:\n",
    "    print(\"Already Parsed Document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorDB 조회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_chroma.vectorstores.Chroma object at 0x00000271BDDA4880>\n",
      "(99, 6)\n",
      "['Rules']\n",
      "['DNV']\n",
      "['DNV Rules for Classification of Ships _2016_39_Fishing vessels'\n",
      " 'DNV Rules for Classification of Ships _2016_40_Offshore Service Vessels, Tugs and Special Ships']\n"
     ]
    }
   ],
   "source": [
    "vector_store = Chroma(collection_name=\"collection_01\", persist_directory=db_path, embedding_function=OllamaEmbeddings(model=\"bge-m3:latest\"))\n",
    "df = read_vectordb_as_df(db_path=db_path)\n",
    "print(vector_store)\n",
    "print(df.shape)\n",
    "print(df[\"first_div\"].unique())\n",
    "print(df[\"second_div\"].unique())\n",
    "print(df[\"filename\"].unique()[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'File Name': 'DNV Rules for Classification of Ships _2016_39_Fishing vessels', 'File Path': './Rules/DNV/DNV Rules for Classification of Ships _2016_39_Fishing vessels.pdf', 'First Division': 'Rules', 'Page': 9, 'Second Division': 'DNV'}, page_content='This page explains DNV Rules for Classification of Ships _2016_39_Fishing vessels, that belongs to catogories of Rules and DNV./nRules for Ships, July 2016  Pt.5 Ch.6 Sec.1 General requirements – Page 10 DET NORSKE VERITAS ASare not fully met, or if the design of the weather deck is such that water may be trapped. The stability calculations shall take the effect of this water into account according to the re quirements of 703 to 705. 702  If hatches or similar openings have to be left periodically open during oper ation, the stability calculations shall take the effect of water in th e open compartment(s) in to account according to the requirements of 703 to 705, provided that the angle of downflooding for the critical opening is less than 30°. Fig. 1 Water on deck criterion 703  The ability of the vessel to withstand the heelin g effect due to the presence of water on deck, is to be demonstrated by a quasi-static method. With reference to Fig. 1, the followi ng criterion is to be satisfied with the vessel in the wo rst operating condition: — area «b» shall be equal to or greater than area «a».The angle that'),\n",
       "  0.2471180926623927),\n",
       " (Document(metadata={'File Name': 'DNV Rules for Classification of Ships _2016_39_Fishing vessels', 'File Path': './Rules/DNV/DNV Rules for Classification of Ships _2016_39_Fishing vessels.pdf', 'First Division': 'Rules', 'Page': 8, 'Second Division': 'DNV'}, page_content=\"This page explains DNV Rules for Classification of Ships _2016_39_Fishing vessels, that belongs to catogories of Rules and DNV./n(if consistent with fishing method) 402  Special loading conditions ass ociated with a change in the vessel's mode or ar ea of operation which affect the stability, are to be considered. 403  If water ballast must be filled between departure and arrival in order to meet the stability criteria, a loading condition is to be in cluded showing when the wate r ballast must be taken on board. The condition is to show the situation just before ba llasting, with the maximum free surfa ce moments of the ballast tank(s) included. 404  Allowance for the weight of wet fi shing net and tackle on deck, is to be included if applicable. 405  Allowance for ice accretion according to 501 must be shown in the worst operating condition in the stability booklet, if consiste nt with area of operation. 406  Homogeneous distribution of catch in all holds, hatch coamings and trunks is to be assumed, unless this is inconsistent with practice. (Volumetric centre of gr avity and identical specific gravity for\"),\n",
       "  0.24270111139012318),\n",
       " (Document(metadata={'File Name': 'DNV Rules for Classification of Ships _2016_40_Offshore Service Vessels, Tugs and Special Ships', 'File Path': './Rules/DNV/DNV Rules for Classification of Ships _2016_40_Offshore Service Vessels, Tugs and Special Ships.pdf', 'First Division': 'Rules', 'Page': 3, 'Second Division': 'DNV'}, page_content=\"This page explains DNV Rules for Classification of Ships _2016_40_Offshore Service Vessels, Tugs and Special Ships, that belongs to catogories of Rules and DNV./n................................................... ................................................... ............ 15 A 200 Documentation requirements............................. ................................................... .............................................. 15 B. Hull Arrangement and Strength....................... ................................................... ................................................... ... 16 B 100 Ship's sides and stern ............................. ................................................... ................................................... ....... 16 B 200 Weather deck for cargo.............................. ................................................... ................................................... ... 16 B 300 Weathertight doors.................................. ................................................... ................................................... ...... 17 B 400 Freeing ports and\"),\n",
       "  0.2302537849254348)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "what is the obligation of the master in a troubled vessel in singapore port?\n",
    "\"\"\"\n",
    "res = vector_store.similarity_search_with_relevance_scores(query=query, k=3)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문서 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "try:\n",
    "  filename = 'Port Information Guide_Rotterdam_2024'\n",
    "  delete_document(filename=filename, db_path=db_path)\n",
    "  vector_store = Chroma(collection_name=\"collection_01\", persist_directory=db_path, embedding_function=OllamaEmbeddings(model=\"bge-m3:latest\"))\n",
    "  df = read_vectordb_as_df(db_path=db_path)\n",
    "\n",
    "  print(df.shape)\n",
    "except:\n",
    "  print(\"문서가 없습니다.\")\n",
    "  vector_store = Chroma(collection_name=\"collection_01\", persist_directory=db_path, embedding_function=OllamaEmbeddings(model=\"bge-m3:latest\"))\n",
    "  df = read_vectordb_as_df(db_path=db_path)\n",
    "  print(df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
